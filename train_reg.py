
import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import pandas as pd
import matplotlib.pyplot as plt
from model_reg import RegressionNetwork
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm

# =========================
# è®¾å¤‡
# =========================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# =========================
# åŠ æƒ MSEï¼ˆä¸åŸç‰ˆä¸€è‡´ï¼‰
# =========================
def weighted_mse_loss(pred, target, base_weight=1.0, angle_weight=10.0):
    """
    ç»™è½¬è§’ä¸ä¸º0çš„æ•°æ®ä¸€ä¸ªæ›´é«˜çš„æƒé‡ï¼Œç”¨äºå¤„ç†æ•°æ®ä¸å¹³è¡¡ã€‚
    æƒé‡ = base_weight + angle_weight * abs(çœŸå®è§’åº¦/30)   # å½’ä¸€åŒ–åˆ°[-1,1]èŒƒå›´çš„æƒé‡å› å­
    """
    weights = base_weight + angle_weight * torch.abs(target) / 30.0
    loss = weights * (pred - target) ** 2
    return loss.mean()


# =========================
# æ•°æ®é›†ï¼ˆä¸åŸç‰ˆä¸€è‡´ï¼‰
# =========================
class LidarRegressionDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32).squeeze()

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        x = self.X[idx]
        y = self.y[idx]
        return x, y


# =========================
# è¯„ä¼°ï¼šLoss / MAE / Hit@3Â°
# =========================
@torch.no_grad()
def evaluate(model, dataloader,
             base_weight=1.0, angle_weight=10.0,
             hit_threshold_deg=3.0):
    model.eval()
    total_loss = 0.0
    total_mae = 0.0
    total_hit = 0.0
    total_samples = 0

    for data, target in dataloader:
        data, target = data.to(device), target.to(device)
        outputs = model(data).squeeze()

        loss = weighted_mse_loss(outputs, target,
                                 base_weight=base_weight,
                                 angle_weight=angle_weight)

        mae = torch.sum(torch.abs(outputs - target)).item()
        hit = torch.sum((torch.abs(outputs - target) < hit_threshold_deg).float()).item()

        bs = target.size(0)
        total_loss += loss.item() * bs
        total_mae += mae
        total_hit += hit
        total_samples += bs

    avg_loss = total_loss / max(total_samples, 1)
    avg_mae = total_mae / max(total_samples, 1)
    hit_rate = total_hit / max(total_samples, 1)
    return avg_loss, avg_mae, hit_rate


# =========================
# è®­ç»ƒä¸»å¾ªç¯ï¼ˆåŠ å…¥éªŒè¯/æ—©åœ/è°ƒåº¦/è®°å½•å†å²ï¼‰
# =========================
def train(model, train_data, val_data,
          num_epochs=1000, batch_size=64, learning_rate=1e-3,
          early_stop_patience=50,
          base_weight=1.0, angle_weight=10.0):

    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)

    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)

    history = {
        "train_loss": [],
        "val_loss": [],
        "val_mae": [],
        "val_hit3": [],
        "lr": []
    }

    best_val_loss = float('inf')
    best_state = None
    patience = 0

    for epoch in tqdm(range(1, num_epochs + 1), desc="Training Progress"):
        model.train()
        running_loss = 0.0
        seen = 0

        for data, target in train_loader:
            data, target = data.to(device), target.to(device)

            optimizer.zero_grad()
            outputs = model(data).squeeze()
            loss = weighted_mse_loss(outputs, target,
                                     base_weight=base_weight,
                                     angle_weight=angle_weight)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            bs = data.size(0)
            running_loss += loss.item() * bs
            seen += bs

        train_loss = running_loss / max(seen, 1)

        # éªŒè¯
        val_loss, val_mae, val_hit3 = evaluate(model, val_loader,
                                               base_weight=base_weight,
                                               angle_weight=angle_weight,
                                               hit_threshold_deg=3.0)

        # å­¦ä¹ ç‡è°ƒåº¦çœ‹ val_loss
        scheduler.step(val_loss)

        # è®°å½•å†å²
        history["train_loss"].append(train_loss)
        history["val_loss"].append(val_loss)
        history["val_mae"].append(val_mae)
        history["val_hit3"].append(val_hit3)
        history["lr"].append(optimizer.param_groups[0]['lr'])

        print(f"Epoch {epoch} | Train Loss: {train_loss:.4f} | "
              f"Val Loss: {val_loss:.4f} | Val MAE(deg): {val_mae:.3f} | "
              f"Hit@3Â°: {val_hit3*100:.1f}% | LR: {optimizer.param_groups[0]['lr']:.2e}")

        # æ—©åœ
        if val_loss < best_val_loss - 1e-4:
            best_val_loss = val_loss
            best_state = model.state_dict()
            patience = 0
            os.makedirs('./model', exist_ok=True)
            torch.save(best_state, './model/model_regression_best.pth')
            print(f"âœ… Best model updated and saved at epoch {epoch}")
        else:
            patience += 1
            if patience >= early_stop_patience:
                print(f"â¹ Early stopping at epoch {epoch} (no improvement for {early_stop_patience} epochs)")
                break

    # æ¢å¤æœ€ä½³å¹¶ä¿å­˜æœ€ç»ˆ
    if best_state is not None:
        model.load_state_dict(best_state)
    os.makedirs('./model', exist_ok=True)
    torch.save(model.state_dict(), './model/model_regression_last.pth')
    print("ğŸ¯ Final model saved.")
    return history


# =========================
# ç”»å›¾
# =========================
def plot_history(history, out_path='./model/training_curves.png'):
    os.makedirs(os.path.dirname(out_path), exist_ok=True)

    epochs = np.arange(1, len(history["train_loss"]) + 1)

    # æ›²çº¿1ï¼šLoss
    plt.figure()
    plt.plot(epochs, history["train_loss"], label='Train Loss')
    plt.plot(epochs, history["val_loss"], label='Val Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Loss vs. Epoch')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(out_path.replace('.png', '_loss.png'))
    plt.close()

    # æ›²çº¿2ï¼šVal MAE
    plt.figure()
    plt.plot(epochs, history["val_mae"], label='Val MAE (deg)')
    plt.xlabel('Epoch')
    plt.ylabel('MAE (deg)')
    plt.title('Validation MAE vs. Epoch')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(out_path.replace('.png', '_mae.png'))
    plt.close()

    # æ›²çº¿3ï¼šHit@3Â°
    plt.figure()
    plt.plot(epochs, history["val_hit3"], label='Hit@3Â°')
    plt.xlabel('Epoch')
    plt.ylabel('Hit Rate')
    plt.title('Hit@3Â° vs. Epoch')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(out_path.replace('.png', '_hit3.png'))
    plt.close()

    # æ›²çº¿4ï¼šLR
    plt.figure()
    plt.plot(epochs, history["lr"], label='Learning Rate')
    plt.xlabel('Epoch')
    plt.ylabel('LR')
    plt.title('Learning Rate vs. Epoch')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(out_path.replace('.png', '_lr.png'))
    plt.close()


# =========================
# æ•°æ®è½½å…¥
# =========================
def load_split(prefix):
    """
    çº¦å®šï¼š
      ./mydata/X_{prefix}.csv               -> LiDAR 360
      ./mydata/direction/Y_{prefix}.csv    -> è§’åº¦ï¼ˆåº¦ï¼‰
      ./mydata/type/Y_{prefix}.csv         -> é“è·¯ç±»å‹ï¼ˆå¯é€‰ï¼Œæœªä½¿ç”¨ï¼‰
      ./mydata/towards/Y_{prefix}.csv      -> è½¬å‘æ–¹å‘ï¼ˆå¯é€‰ï¼Œæœªä½¿ç”¨ï¼‰
    å¯¹é½æ—§ç‰ˆï¼šX = [X_main, path_type, turn_direction] (å…± 362ç»´)
    """
    X_main = pd.read_csv(f'./mydata/X_{prefix}.csv', header=None).values
    path_type = pd.read_csv(f'./mydata/type/Y_{prefix}.csv', header=None).values
    turn_direction = pd.read_csv(f'./mydata/towards/Y_{prefix}.csv', header=None).values
    X = np.hstack([X_main, path_type, turn_direction])
    y = pd.read_csv(f'./mydata/direction/Y_{prefix}.csv', header=None).values
    return X, y


# =========================
# å…¥å£
# =========================
if __name__ == '__main__':
    # è¯»å–è®­ç»ƒ/éªŒè¯é›†ï¼ˆtrain/test å‘½åä¸å¦ä¸€ä¸ªè„šæœ¬ä¿æŒä¸€è‡´ï¼‰
    X_train, y_train = load_split('train')
    X_val,   y_val   = load_split('test')

    # æ„å»ºæ•°æ®é›†/åŠ è½½å™¨
    train_dataset = LidarRegressionDataset(X_train, y_train)
    val_dataset   = LidarRegressionDataset(X_val,   y_val)

    # åˆå§‹åŒ–æ¨¡å‹ï¼ˆä¿æŒä½ åŸæœ¬çš„ RegressionNetwork å®šä¹‰ï¼‰
    model = RegressionNetwork().to(device)

    # è®­ç»ƒ
    history = train(model, train_dataset, val_dataset,
                    num_epochs=3000,
                    batch_size=64,
                    learning_rate=1e-3,
                    early_stop_patience=100,
                    base_weight=1.0,
                    angle_weight=10.0)

    # ç”»æ›²çº¿
    plot_history(history, out_path='./model/training_curves.png')
    print('ğŸ“ˆ Curves saved to ./model/training_curves_*')
