# train_regression.py
import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm

from model_reg import RegressionNetwork

# ============== è®¾å¤‡/éšæœºç§å­ ==============
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def set_seed(seed=42):
    import random
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(2025)

# ============== åŠ æƒ MSE ==============
def weighted_mse_loss(pred, target, base_weight=1.0, angle_weight=10.0):
    """
    ç»™è½¬è§’ä¸ä¸º0çš„æ•°æ®æ›´é«˜æƒé‡:
    weight = base_weight + angle_weight * |target| / 30
    """
    weights = base_weight + angle_weight * torch.abs(target) / 30.0
    loss = weights * (pred - target) ** 2
    return loss.mean()

# ============== æ•°æ®é›† ==============
class LidarRegressionDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32).squeeze()

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# ============== è¯„ä¼° ==============
@torch.no_grad()
def evaluate(model, dataloader,
             base_weight=1.0, angle_weight=10.0,
             hit_threshold_deg=3.0):
    model.eval()
    total_loss, total_mae, total_hit, total_samples = 0.0, 0.0, 0.0, 0

    for data, target in dataloader:
        data, target = data.to(device), target.to(device)
        outputs = model(data).squeeze()

        loss = weighted_mse_loss(outputs, target,
                                 base_weight=base_weight,
                                 angle_weight=angle_weight)

        mae = torch.sum(torch.abs(outputs - target)).item()
        hit = torch.sum((torch.abs(outputs - target) < hit_threshold_deg).float()).item()

        bs = target.size(0)
        total_loss += loss.item() * bs
        total_mae  += mae
        total_hit  += hit
        total_samples += bs

    avg_loss = total_loss / max(total_samples, 1)
    avg_mae  = total_mae  / max(total_samples, 1)
    hit_rate = total_hit   / max(total_samples, 1)
    return avg_loss, avg_mae, hit_rate

# ============== è®­ç»ƒä¸»å¾ªç¯ ==============
def train(model, train_data, val_data,
          num_epochs=1000, batch_size=64, learning_rate=1e-3,
          early_stop_patience=50,
          base_weight=1.0, angle_weight=10.0):

    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)
    val_loader   = DataLoader(val_data,   batch_size=batch_size, shuffle=False)

    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=10, verbose=False
    )

    history = {"train_loss": [], "val_loss": [], "val_mae": [], "val_hit3": [], "lr": []}

    best_val_loss = float('inf')
    best_state = None
    patience = 0

    for epoch in tqdm(range(1, num_epochs + 1), desc="Training Progress"):
        model.train()
        running_loss, seen = 0.0, 0

        for data, target in train_loader:
            data, target = data.to(device), target.to(device)

            optimizer.zero_grad()
            outputs = model(data).squeeze()
            loss = weighted_mse_loss(outputs, target,
                                     base_weight=base_weight,
                                     angle_weight=angle_weight)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            bs = data.size(0)
            running_loss += loss.item() * bs
            seen += bs

        train_loss = running_loss / max(seen, 1)

        # éªŒè¯
        val_loss, val_mae, val_hit3 = evaluate(
            model, val_loader,
            base_weight=base_weight, angle_weight=angle_weight, hit_threshold_deg=3.0
        )

        scheduler.step(val_loss)

        # è®°å½•
        history["train_loss"].append(train_loss)
        history["val_loss"].append(val_loss)
        history["val_mae"].append(val_mae)
        history["val_hit3"].append(val_hit3)
        history["lr"].append(optimizer.param_groups[0]['lr'])

        print(f"Epoch {epoch} | Train Loss: {train_loss:.4f} | "
              f"Val Loss: {val_loss:.4f} | Val MAE(deg): {val_mae:.3f} | "
              f"Hit@3Â°: {val_hit3*100:.1f}% | LR: {optimizer.param_groups[0]['lr']:.2e}")

        # æ—©åœ
        if val_loss < best_val_loss - 1e-4:
            best_val_loss = val_loss
            best_state = model.state_dict()
            patience = 0
            os.makedirs('./model', exist_ok=True)
            torch.save(best_state, './model/model_regression_best.pth')
            print(f"âœ… Best model updated and saved at epoch {epoch}")
        else:
            patience += 1
            if patience >= early_stop_patience:
                print(f"â¹ Early stopping at epoch {epoch} "
                      f"(no improvement for {early_stop_patience} epochs)")
                break

    # æ¢å¤æœ€ä½³å¹¶ä¿å­˜æœ€ç»ˆ
    if best_state is not None:
        model.load_state_dict(best_state)
    os.makedirs('./model', exist_ok=True)
    torch.save(model.state_dict(), './model/model_regression_last.pth')
    print("ğŸ¯ Final model saved.")
    return history

# ============== ç”»å›¾ ==============
def plot_history(history, out_path='./model/training_curves.png'):
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    epochs = np.arange(1, len(history["train_loss"]) + 1)

    # Loss
    plt.figure()
    plt.plot(epochs, history["train_loss"], label='Train Loss')
    plt.plot(epochs, history["val_loss"], label='Val Loss')
    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Loss vs. Epoch')
    plt.legend(); plt.grid(True); plt.tight_layout()
    plt.savefig(out_path.replace('.png', '_loss.png')); plt.close()

    # Val MAE
    plt.figure()
    plt.plot(epochs, history["val_mae"], label='Val MAE (deg)')
    plt.xlabel('Epoch'); plt.ylabel('MAE (deg)'); plt.title('Validation MAE vs. Epoch')
    plt.legend(); plt.grid(True); plt.tight_layout()
    plt.savefig(out_path.replace('.png', '_mae.png')); plt.close()

    # Hit@3Â°
    plt.figure()
    plt.plot(epochs, history["val_hit3"], label='Hit@3Â°')
    plt.xlabel('Epoch'); plt.ylabel('Hit Rate'); plt.title('Hit@3Â° vs. Epoch')
    plt.legend(); plt.grid(True); plt.tight_layout()
    plt.savefig(out_path.replace('.png', '_hit3.png')); plt.close()

    # LR
    plt.figure()
    plt.plot(epochs, history["lr"], label='Learning Rate')
    plt.xlabel('Epoch'); plt.ylabel('LR'); plt.title('Learning Rate vs. Epoch')
    plt.legend(); plt.grid(True); plt.tight_layout()
    plt.savefig(out_path.replace('.png', '_lr.png')); plt.close()

# ============== è¯»å–å¹¶æ‹¼æ¥æˆ 362 ç»´ ==============
def load_split(prefix):
    """
    è¯»å–å¹¶æŒ‰è¡Œæ‹¼æ¥ï¼š
      X_main:            ./mydata/X_{prefix}.csv            -> [N, 360]
      path_type:         ./mydata/type/Y_{prefix}.csv       -> [N, 1]
      turn_direction:    ./mydata/towards/Y_{prefix}.csv    -> [N, 1]
      y(è§’åº¦):           ./mydata/direction/Y_{prefix}.csv  -> [N, 1]
    è¿”å›:
      X: [N, 362], y: [N, 1]
    """
    X_main = pd.read_csv(f'./mydata/X_{prefix}.csv', header=None).values.astype(np.float32)
    path_type = pd.read_csv(f'./mydata/type/Y_{prefix}.csv', header=None).values.astype(np.float32)
    turn_direction = pd.read_csv(f'./mydata/towards/Y_{prefix}.csv', header=None).values.astype(np.float32)
    y = pd.read_csv(f'./mydata/direction/Y_{prefix}.csv', header=None).values.astype(np.float32)

    # å½¢çŠ¶æ ¡éªŒä¸æ ‡å‡†åŒ–
    if X_main.ndim != 2 or X_main.shape[1] != 360:
        raise ValueError(f'X_{prefix}.csv å½¢çŠ¶å¼‚å¸¸ï¼ŒæœŸæœ› [N,360]ï¼Œå®é™… {X_main.shape}')
    if path_type.ndim == 1:
        path_type = path_type.reshape(-1, 1)
    if turn_direction.ndim == 1:
        turn_direction = turn_direction.reshape(-1, 1)
    if y.ndim == 1:
        y = y.reshape(-1, 1)

    n = X_main.shape[0]
    if not (path_type.shape[0] == n and turn_direction.shape[0] == n and y.shape[0] == n):
        raise ValueError(f'è¡Œæ•°ä¸ä¸€è‡´ï¼šX={n}, type={path_type.shape[0]}, '
                         f'towards={turn_direction.shape[0]}, y={y.shape[0]}')

    # æŒ‰è¡Œæ‹¼æ¥ -> [N, 362]
    X = np.hstack([X_main, path_type, turn_direction]).astype(np.float32)
    return X, y

# ============== å…¥å£ ==============
if __name__ == '__main__':
    # è¯»å–è®­ç»ƒ/éªŒè¯é›†
    X_train, y_train = load_split('train')
    X_val,   y_val   = load_split('test')

    print('Train shapes:', X_train.shape, y_train.shape)  # (N_train, 362) (N_train, 1)
    print('Val   shapes:', X_val.shape,   y_val.shape)    # (N_val,   362) (N_val,   1)

    # æ•°æ®é›†/åŠ è½½å™¨
    train_dataset = LidarRegressionDataset(X_train, y_train)
    val_dataset   = LidarRegressionDataset(X_val,   y_val)

    # åˆå§‹åŒ–æ¨¡å‹
    model = RegressionNetwork(dropout_p=0.3).to(device)

    # è®­ç»ƒ
    history = train(model, train_dataset, val_dataset,
                    num_epochs=1000,
                    batch_size=64,
                    learning_rate=1e-3,
                    early_stop_patience=100,
                    base_weight=1.0,
                    angle_weight=10.0)

    # ç”»æ›²çº¿
    plot_history(history, out_path='./model/training_curves.png')
    print('ğŸ“ˆ Curves saved to ./model/training_curves_*')
